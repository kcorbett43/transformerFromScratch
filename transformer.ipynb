{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f48e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bcb06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06003b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_text = \"\"\"Initially only available in English, editions of Wikipedia in more than 300 other languages\n",
    "have been developed. The English Wikipedia, with its over 6.9 million articles, is the largest of the editions,\n",
    "which together comprise more than 64 million articles and attract more than 1.5 billion unique device visits and\n",
    "13 million edits per month (about 5 edits per second on average) as of April 2024.[W 1] As of November 2024, over\n",
    "25% of Wikipedia's traffic was from the United States, followed by Japan at 6.2%, the United Kingdom at 5.6%, Russia\n",
    " at 5.0%, Germany at 4.8%, and the remaining 53.3% split among other countries.[8]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(wiki_text).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be00548",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(wiki_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get top tokens\n",
    "top_tokens = [token for token, _ in tokens[:12]]\n",
    "\n",
    "# Combine them into a single string\n",
    "combined_string = tokenizer.convert_tokens_to_string(top_tokens)\n",
    "\n",
    "print(combined_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b498ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "print(f\"size of our base vocabulary: {len(base_vocab)}\")\n",
    "print(f'first element: {base_vocab[0]}, last element: {base_vocab[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eeadd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa54ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, seq_len, n=10000):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.encoding = torch.zeros((seq_len, d_model))\n",
    "    for pos in range(seq_len):\n",
    "      for i in range(int(d_model/2)):\n",
    "        wave_input = pos/(n**(2*i/d_model))\n",
    "        self.encoding[pos,2*i] = math.sin(wave_input)\n",
    "        self.encoding[pos,2*i+1] = math.cos(wave_input)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "    # Dynamically slice positional encoding to match input sequence length\n",
    "    pos_encoding = self.encoding[:seq_len, :].unsqueeze(0)\n",
    "    pos_encoding = pos_encoding.to(x.device)\n",
    "\n",
    "    return x + pos_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f458709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self, d_model, d_k, d_v):\n",
    "    super(Attention, self).__init__()\n",
    "    self.W_Q = nn.Linear(d_model, d_k)\n",
    "    self.W_K = nn.Linear(d_model, d_k)\n",
    "    self.W_V = nn.Linear(d_model, d_v)\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, X, mask=None):\n",
    "    Q = self.W_Q(X)\n",
    "    K = self.W_K(X)\n",
    "    V = self.W_V(X)\n",
    "\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "    d_k = Q.size(-1)\n",
    "    scaled_scores = attention_scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    attention_weights = self.softmax(scaled_scores)\n",
    "\n",
    "    attention_output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    if mask is not None:\n",
    "      scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "    return attention_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Define learnable projection matrices for Q, K, V\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size, seq_len, d_model = X.shape\n",
    "\n",
    "        # 1. Project input to Q, K, V spaces\n",
    "        Q = self.W_Q(X)\n",
    "        K = self.W_K(X)\n",
    "        V = self.W_V(X)\n",
    "\n",
    "        # 2. Split Q, K, V into multiple heads\n",
    "        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores: QK^T / sqrt(d_k)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, seq_len, d_model)\n",
    "        output = self.W_O(attention_output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672560a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim, dropout_rate=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.ln1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.ln2 = nn.Linear(ff_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ln2(self.dropout(torch.relu(self.ln1(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e223ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super(EncoderTransformer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.feed_forward = FeedForward(embed_dim, ff_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.attention(x, mask)\n",
    "\n",
    "        x = x + attn_out\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = x + ff_out\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607dc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        self.masked_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.feed_forward = FeedForward(embed_dim, ff_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, encoder_output, tgt_mask=None, memory_mask=None):\n",
    "        attn_out = self.masked_attention(x, tgt_mask)\n",
    "\n",
    "        x = x + attn_out\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = x + ff_out\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cacab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, embed_dim, num_heads, num_layers, ff_dim):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_encoding = PositionalEncoding(embed_dim, max_len)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [DecoderTransformer(embed_dim, num_heads, ff_dim) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, mask=None):\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.position_encoding(x)\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        logits = self.fc_out(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84982750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 50257  # GPT-2 tokenizer's vocabulary size\n",
    "embed_dim = 768\n",
    "num_heads = 6\n",
    "num_layers = 6\n",
    "ff_dim = 3072\n",
    "max_len = 1024\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize and preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=max_len, padding=\"max_length\")\n",
    "\n",
    "streamed_dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\", streaming=True).shuffle(buffer_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1084f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2(vocab_size, max_len, embed_dim, num_heads, num_layers, ff_dim).to(device)  # Move model to GPU\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2bdf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766dbba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_to_process = 12800\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    processed = 0\n",
    "    batch = []\n",
    "    streamed_dataset = streamed_dataset.shuffle()\n",
    "    for example in streamed_dataset:\n",
    "      batch.append(example[\"text\"])\n",
    "      processed += 1\n",
    "      if len(batch) == batch_size:\n",
    "        # Shuffle batch\n",
    "        random.shuffle(batch)\n",
    "\n",
    "        # Tokenize and process\n",
    "        tokenized = tokenizer(\n",
    "          batch,\n",
    "          truncation=True,\n",
    "          max_length=max_len,\n",
    "          padding=\"max_length\",\n",
    "          return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"].to(device)\n",
    "        attention_mask = tokenized[\"attention_mask\"].to(device)\n",
    "        labels = input_ids.clone().to(device)\n",
    "\n",
    "        # Training step\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        batch = []\n",
    "        if batch_count % 10 == 0:  # Print every 10 batches\n",
    "          print(f\"Epoch {epoch + 1}, Batch {batch_count}, Loss: {loss.item():.4f}\")\n",
    "      if processed > max_to_process:\n",
    "        print(\"processed over max\")\n",
    "        break\n",
    "    if batch_count > 0:\n",
    "      avg_loss = total_loss / batch_count\n",
    "      print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "    else:\n",
    "      print(\"help\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4edd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_full3.pth')  # Save the entire model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae2a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load('model_full3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6416d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = model.to(device)  # Move the model to the specified device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d007d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=10.0, top_k=50):\n",
    "    model.eval()\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate tokens iteratively\n",
    "    for _ in range(max_length):\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs[:, -1, :]\n",
    "        logits = logits / temperature\n",
    "\n",
    "        top_k_values, top_k_indices = torch.topk(logits, top_k, dim=-1)\n",
    "        probabilities = torch.softmax(top_k_values, dim=-1)\n",
    "\n",
    "        next_token_index = torch.multinomial(probabilities, num_samples=1)\n",
    "        next_token = top_k_indices.gather(dim=-1, index=next_token_index)\n",
    "\n",
    "        # Append the token to input\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        # Stop generation if EOS token is generated\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Decode the generated tokens into text\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"My name is\"\n",
    "generated_text = generate_text(model, tokenizer, prompt, max_length=50, temperature=5, top_k=10)\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e12d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "torch.save(model, '/content/drive/My Drive/model_full2.pth')\n",
    "\n",
    "torch.save(model.state_dict(), '/content/drive/My Drive/model_weights2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd8b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
